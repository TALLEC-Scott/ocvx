{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP I : Descentes de Gradient\n",
    "\n",
    "Ce TP vise √† apporter les √©l√©ments n√©cessaires pour comprendre les implementations des descentes de gradients. **C'est un *TP √† trous* ; il s'agira de compl√©ter ces trous et d'y ajouter les tests qui vous sembleront utiles.**\n",
    "\n",
    "Voici un aper√ßu des points abord√©s lors de ce TP.\n",
    "\n",
    "- partie I\n",
    "    - D√©finition d'un ensemble de fonctions test\n",
    "- partie II\n",
    "    - Calcul du gradient d'une fonction de mani√®re approch√©e (pour couvrir des cas o√π le calcul explicite du gradient est impossible ou p√©nible).\n",
    "- partie III\n",
    "    - La descente dans la direction du Gradient √† pas constant\n",
    "    - Optimisation du pas par Backtracking\n",
    "- partie IV\n",
    "    - Choix d'une autre direction que le gradient\n",
    "        - De plus forte pente en norme $l_1$\n",
    "        - Gradient conjugu√©\n",
    "- partie V\n",
    "    - Acc√©l√©ration : Momentum, Nesterov, Adagrda.\n",
    "- partie VI\n",
    "    - La M√©thode de Newton et la m√©thode de quasi-Newton\n",
    "\n",
    "    \n",
    "Dans l'ensemble du d√©roul√© du TP vous ferez bien attention √† valider par un jeu de tests la validit√© des programmes √©crits. Vous regarderez l'influence des param√®tres sur la convergence. Vous comparerez les int√©r√™ts des dif√©rentes m√©thodes les unes par rapport aux autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attendus de rendu\n",
    "\n",
    "Jusqu'il n'y a pas tr√®s longtemps (l'ann√©e derni√®re, pour √™tre pr√©cis), les √©tudiants √©taient √©valu√©s sur leur capacit√© √† impl√©menter des m√©thodes de descentes op√©rationnelles et √† les comparer entre elles.<br>\n",
    "Il est aujourd'hui tr√®s facile d'impl√©menter n'importe quel algorithme de descente, en n'importe quel langage (merci ChatGPT). Les attendus sont donc amen√©s √† changer : exit la v√©rification que le code que vous avez √©crit est correct (car il n'est plus possible d'en garantir la source (sachant que √ßa n'√©tait d√©j√† pas √©vident par le pass√©...)).\n",
    "\n",
    "Cependant, la capacit√© √† produire un benchmark de qualit√©, et surtout √† l'analyser de mani√®re rigoureuse et d'en tirer des conclusions pertinentes, reste quand m√™me propre √† l'humain.\n",
    "\n",
    "**Votre rendu sera donc jug√© sur** :\n",
    "- l'√©tude effectu√©e concernant la sensibilit√© de vos algorithmes de descente aux hyperparam√®tres / conditions initiales\n",
    "- l'analyse comparative propos√©e quant aux diff√©rentes impl√©mentations sugg√©r√©es \n",
    "- la pr√©cision et concision des r√©sultats pr√©sent√©s (par exemple, dans les choix que vous ferez pour visualiser l'influence de tel ou tel param√®tre sur telle ou telle m√©trique attestant de la convergence de votre algorithme).\n",
    "\n",
    "Comme vous allez le d√©couvrir, ce TP est tr√®s riche et tr√®s long. Il n'est pas attendu de vous que vous impl√©mentiez n√©c√©ssairement **toutes** les m√©thodes demand√©es, ni que vous benchmarkiez pour une m√©thode donn√©e, l'influence de **tous** les hyperparam√®tres. C'est √† vous de choisir ce que vous voulez comparer, en fonction de ce qui a le plus √©veill√© votre curiosit√© lors des cours en lien avec les m√©thodes de descente.\n",
    "\n",
    "√Ä titre d'exemple, voici le genre de *benchmarks* que vous pouvez faire en fonction de leur difficult√© (symbolis√©e par le nombre de ‚≠ê):\n",
    "- Influence du pas sur le nombre d'it√©rations dans le cas de la descente de gradient √† pas constant et ad√©quation avec la th√©orie (‚≠ê)\n",
    "- Influence d'un hyperparam√®tre du crit√®re d'Armijo dans le cas d'une descente de gradient pour une fonction convexe (‚≠ê)\n",
    "- Influence du crit√®re d'arr√™t sur le nombre d'it√©rations de la m√©thode de descente (‚≠ê)\n",
    "- Influence conjointe des deux hyperparam√®tres du crit√®re d'Armijo dans le cas d'une descente de gradient pour une fonction convexe (‚≠ê‚≠ê)\n",
    "- Influence du choix de la m√©thode (Fletcher-Reeves vs Polack-Ribi√®re) pour l'algorithme du gradient conjugu√© pour une classe de fonction non-convexes (‚≠ê‚≠ê)\n",
    "- Comparaison de rapidit√© et de pr√©cision pour les m√©thodes de gradient conjugu√©, Newton (dans le cas quadratique) et quasi-Newton dans le cas g√©n√©ral (‚≠ê‚≠ê‚≠ê)\n",
    "\n",
    "√âvidemment, libre √† vous de conduire n'importe quel autre type de benchmark, du moment que votre choix est justifi√©.\n",
    "\n",
    "‚ö†Ô∏è Un benchmark bien conduit, c'est bien. Mais un benchmark bien conduit **et** bien analys√©, c'est mieux ! Pour chaque benchmark que vous produirez, chaque courbe que vous tracerez, vous devez donc vous poser la question \"Que puis-je en d√©duire ? Est-ce conforme √† ce que pr√©dit la th√©orie (ou √† minima, l'intuition que j'ai du fonctionnement de la m√©thode qui est benchmark√©e) ?\n",
    "\n",
    "Ce TP est √† rendre par **groupe de 2** ou **par groupe de 3**.<br>\n",
    "Dans tous les cas, vous devrez rendre un rapport au format pdf regroupant tous les benchmarks et analyses que vous voudrez pr√©senter :\n",
    "- 10 $\\pm$ 1 pages si vous travaillez √† 2\n",
    "- 15 $\\pm$ 1 pages si vous travaillez √† 3\n",
    "\n",
    "Des p√©nalit√©s s'appliqueront sur la notation pour tout rapport trop court ou trop long. **Dans tous les cas, vous indiquerez la r√©partition du travail au sein du groupe** (qui a r√©alis√© quel benchmark, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Au travail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Un set de fonctions tests\n",
    "\n",
    "Cette partie ne n√©cessite pas de travail de votre part\n",
    "\n",
    "Nous allons ici introduire quelques fonctions test qui permettent de repr√©senter les situations suivantes:\n",
    "- Fonctions globalement convexes ou uniquement localement convexes\n",
    "- Fonctions admettant ou non un minimum global\n",
    "- Fonctions admettant ou non des minimums locaux\n",
    "\n",
    "Nous partons de fonctions d√©finies sur $\\mathbb{R}$ mais on verra que cela permet de d√©finir des fonctions \"int√©ressantes\" sur $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions du set de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. D√©finir des familles de fonctions *convexes* sur $\\mathbb{R}$  ou sur une partie de $\\mathbb{R}$ ayant un nombre de conditionnement uniquement d√©pendant des param√®tres de la famille.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic1_(x,gamma):\n",
    "    return gamma*(x**2) + x + 1 #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "x = np.linspace(-5, 5, 400)\n",
    "ax.set_ylim(-1, 100)\n",
    "for gamma in range(5, 50, 5):\n",
    "    ax.plot(x, quadratic1_(x, gamma), label=\"gamma: {}\".format(gamma))\n",
    "ax.set_title(\"Famille quadratiques en dimension 1\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic1_(x,gamma):\n",
    "    return x**3 + gamma*x**2 + x + 1 #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "x = np.linspace(-10, 10, 400)\n",
    "ax.set_ylim(-50, 150)\n",
    "for gamma in range(0, 11, 1):\n",
    "    ax.plot(x, cubic1_(x, gamma), label=\"gamma: {}\".format(gamma))\n",
    "ax.set_title(\"Famille cubique en dimension 1\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitrous1_(x,gamma):\n",
    "        return 20*np.cos(x**2) + (gamma * x**2) #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "x = np.linspace(-10, 10, 400)\n",
    "ax.set_ylim(-100, 1000)\n",
    "for gamma in range(1, 11, 1):\n",
    "    ax.plot(x, multitrous1_(x, gamma), label=\"gamma: {}\".format(gamma))\n",
    "ax.set_title(\"Famille multi-puits en dimension 1\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Faire de m√™me avec des fonctions sur $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic2_(x,gamma):\n",
    "        return quadratic1_(x[0],gamma[0])+quadratic1_(x[1],gamma[1]) #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -5, 5, -5, 5\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "gamma=np.zeros(2)\n",
    "gamma[0]=10\n",
    "gamma[1]=1\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=quadratic2_(np.array([X[i,j],Y[i,j]]),gamma)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "\n",
    "ax.contour(X, Y, Z,20) \n",
    "  \n",
    "ax.set_title('fonction quadratic2') \n",
    "ax.set_xlabel('x') \n",
    "ax.set_ylabel('y') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic2_(x,gamma=10):\n",
    "        return cubic1_(x[0],gamma)+cubic1_(x[1],gamma) #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -10, 5, -10, 5\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=cubic2_(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "\n",
    "ax.contour(X, Y, Z,50) \n",
    "  \n",
    "ax.set_title('fonction cubic2') \n",
    "ax.set_xlabel('x') \n",
    "ax.set_ylabel('y') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitrous2_(x,gamma=4):\n",
    "        return multitrous1_(x[0],1)+multitrous1_(x[1],gamma) #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -7.5, 7.5, -7.5, 7.5\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=multitrous2_(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "\n",
    "ax.contour(X, Y, Z,50) \n",
    "  \n",
    "ax.set_title('fonction multitrou2') \n",
    "ax.set_xlabel('x') \n",
    "ax.set_ylabel('y') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. On construit ici une fonction convexe en dimension n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On va prendre comme fonction test la fonction convexe \"g√©n√©rique\" dans $\\mathbb{R}^n$, $ x \\mapsto \\frac{1}{2}x^T A x-b^Tx$, \n",
    "\n",
    "o√π A est une matrice sym√©trique d√©finie positive de taille $(n,n)$ et $b$ un vecteur de $\\mathbb{R}^n$.\n",
    "\n",
    "On a vu en cours que cette fonction est convexe, qu'elle admet donc un minimum global sur $\\mathbb{R}^n$, et que ce minimum est atteint au point o√π son gradient s'annule, c'est √† dire au point o√π $Ax=b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Une mani√®re commode de construire une matrice $A$ sym√©trique d√©finie positive est de la voir comme une matrice de la forme $A=U^T U$ avec $U$ triangulaire sup√©rieure car cela nous permet  √† la fois de s'assurer qu'elle est sym√©trique, inversible, positive et en plus de \"contr√¥ler\" les valeurs propres de $A$ et donc son conditionnement.\n",
    "Le conditionnement de $A$ est le rapport entre la plus grande et la plus petite de ses valeurs propres. C'est le carr√© du rapport entre la plus grande et la plus petite des valeurs propres de $U$ qu'on trouve sur la diagonale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system (dim,cond=10,seed=100):\n",
    "    np.random.seed(seed)\n",
    "    A=0.1*np.random.uniform(-math.sqrt(cond),math.sqrt(cond),size=(dim,dim))\n",
    "    A=np.triu(A)\n",
    "    # on remplace la diagonale de A par des valeurs al√©atoires positives entre 1 et sqrt(cond)\n",
    "    A=A-np.diag(np.diag(A))+np.diag(np.random.uniform(1.,math.sqrt(cond),size=(dim))) \n",
    "    # on impose les deux premiers termes de la diagonale diagonale de A pour fixer le conditionnement\n",
    "    A[0,0]=1.\n",
    "    A[1,1]=math.sqrt(cond)\n",
    "    b=1.*np.random.randint(-10,10,size=(dim))\n",
    "    A=A.T @ A\n",
    "    return A,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,b = create_system(5,cond=10)\n",
    "print (A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadraticn_(x):\n",
    "    return (x.T@A@x)/2-b.T@x #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -20, 10, -10, 10\n",
    "\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "dim=2\n",
    "A,b = create_system(dim,cond=10)\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=quadraticn_(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "\n",
    "ax.contour(X, Y, Z,15) \n",
    "  \n",
    "ax.set_title('quadraticn_') \n",
    "ax.set_xlabel('x') \n",
    "ax.set_ylabel('y') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. La banane de Rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Voici une fonction c√©l√®bre pour tester les algorithmes d'optimisation.\n",
    "\n",
    "$$f(x,y)=(x-1)^2+\\gamma (x^2-y)^2.$$\n",
    "\n",
    "Ce n'est pas √©vident √† visualiser mais cette fonction pr√©sente un minimum global unique qui se situe au fond d'une vall√©e tr√®s √©troite et en forme de parabole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(x,gamma=100):\n",
    "        return (x[0]-1)**2+gamma*(x[0]**2-x[1])**2 #valeur de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -1, 1.5, -0.5, 2\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=Rosenbrock(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "ax.contour(X, Y, Z,100) \n",
    "ax.set_title('fonction Rosenbrock') \n",
    "ax.set_xlabel('x') \n",
    "ax.set_ylabel('y') \n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "cmap='jet', edgecolor='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On voit sur ces quelques exemples que des fonctions d'apparence \"inoffensives\" peuvent avoir des comportements complexes en ce qui concerne leurs extrema locaux ou globaux.\n",
    "\n",
    "${\\bf Nous \\, vous \\, invitons \\, √† \\, tester}$ les programmes que vous √©crirez:\n",
    "* sur la fonction quadratique pour la dimension n=2 puis n=10, et pour des conditionnements variant de 5 √† 1000.\n",
    "* sur la fonction de Rosenbrock pour des valeurs de gamma variant de 5 √† 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II- Diff√©rencier une fonction num√©riquement\n",
    "\n",
    "On verra aussi comment calculer le gradient de mani√®re approch√©e. Cela est particuli√®rement utile quand la seule information disponible sur la fonction √† minimiser est sa valeur en tout point.\n",
    "\n",
    "Pour calculer le gradient d'une fonction on a d√©j√† besoin de savoir calculer la d√©riv√©e d'une fonction r√©elle, et on en tire ensuite une version num√©rique du gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le calcul num√©rique de la d√©riv√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pour $h$ assez petit on peut approcher $f'(x)$ par \n",
    "$$ f'(x) \\simeq \\frac{f(x + h) - f(x)}{h} .$$\n",
    "L'erreur de l'approximation est en $o(1)$ quand $h \\to 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ $$\n",
    "On peut en r√©alit√© faire un peu mieux en approchant $f'(x)$ par \n",
    "$$f'(x) \\simeq \\frac{f(x+h)-f(x-h)}{2h}.$$\n",
    "On trouve que l'erreur d'approximation dans le second cas est d√©sormais en $o(h)$ quand $h \\to 0$; ce qui est a priori meilleur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utiliser la d√©marche pr√©c√©dente pour approcher la d√©riv√©e partielle d'une fonction en un point. Cette fonction sera not√©e `partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(f, x, i=0, dx=1e-8):\n",
    "    \"\"\"Computes i-th partial derivative of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which partial derivative is computed.\n",
    "        i: coordinate along which derivative is computed.\n",
    "        dx: slack for finite difference.\n",
    "        \n",
    "    Output:\n",
    "        (float)\n",
    "\n",
    "    \"\"\"\n",
    "    h = np.zeros(x.size)\n",
    "    h[i] = dx\n",
    "    return (f(x + h) - f(x - h)) / (2*dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Comparer `partial` √† l'expression exacte de la d√©riv√©e partielle d'une fonction de votre choix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial(lambda x: np.exp(x), np.array([100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.array([100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le calcul du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dit pr√©c√©demment, comme le calcul exact du gradient n'est parfois pas possible ou facile, on se garde la possibilit√© de calculer num√©riquement le gradient d'une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. √âcrire une fonction `gradient` qui renvoie le gradient d'une fonction en un point gr√¢ce √† la fonction pr√©c√©dente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, x, dx=1e-8):\n",
    "    \"\"\"Computes gradient of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which gradient is computed.\n",
    "        dx: slack for finite difference of partial derivatives.\n",
    "        \n",
    "    Output:\n",
    "        (ndarray) of size domain of f.\n",
    "        \n",
    "    \"\"\"\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tester cette fonction gradient sur une fonction connue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros(2)\n",
    "x[0]=1\n",
    "x[1]=10\n",
    "gradient(lambda x: x[0]**5 + x[1]**2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## III- Descente du gradient\n",
    "\n",
    "Tout d'abord, nous allons impl√©menter des descentes qui se feront dans la direction du Gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le cas g√©n√©rique de descente de gradient avec un pas constant\n",
    "\n",
    "\n",
    "* prendre un point de d√©part au hasard ${\\bf x_0}$.\n",
    "\n",
    "Quand on est au point ${\\bf x_k}$\n",
    "\n",
    "* calculer le gradient de $f$ en ce point $\\nabla f({\\bf x_k})$\n",
    "\n",
    "* On choisit une direction de descente ${\\bf d}_k = - \\nabla f({\\bf x}_k)$\n",
    "\n",
    "* avancer dans cette direction : ${\\bf x}_{k+1} = {\\bf x}_k + \\eta \\, {\\bf d}_k$\n",
    "\n",
    "et on recommence cette derni√®re √©tape jusqu'√† ce qu'on arrive √† un point fixe c.a.d. que \n",
    "$|| {\\bf x}_{k+1} - {\\bf x}_k|| < \\varepsilon$ avec $\\varepsilon$ une toute petite valeur.\n",
    "\n",
    "Impl√©menter cette m√©thode, la tester sur des fonctions tests d√©finies plus haute, observer en particulier le comportement en fonction de $\\mu$ qui est le pas constant.\n",
    "\n",
    "Attention: bien pr√©voir dans le code des conditions pour √©viter de se retrouver pi√©g√© dans des boucles infinies !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Impl√©menter cette m√©thode, la tester sur des fonctions tests d√©finies plus haute, observer en particulier le comportement en fonction de $\\eta$ qui est le pas constant.\n",
    "\n",
    "Attention: bien pr√©voir dans le code des conditions pour √©viter de se retrouver pi√©g√© dans des boucles infinies !!!\n",
    "On conseille de stocker les diff√©rentes valeurs calcul√©es pour les points ${\\bf x_k}$ afin de pouvoir \"regarder\" la mani√®re dont la convergence se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_grad_const (f,x0,eta=0.001,eps=1E-6):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    x0: point initial\n",
    "    f: fonction √† minimiser\n",
    "    eta: valeur constante du pas (0.001 par d√©faut)\n",
    "    eps: crit√®re √† partir duquel on consid√®rera que la suite est \"constante\"\n",
    "    \n",
    "    Output\n",
    "    Tableau contenant l'ensemble des positions successives p_k\n",
    "    \"\"\"\n",
    "    \n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=10\n",
    "cond=100.\n",
    "A,b = create_system(dim,cond)\n",
    "x_exact=np.zeros(dim)+1.\n",
    "b=A@x_exact\n",
    "\n",
    "x0=np.zeros(dim)\n",
    "res = desc_grad_const(quadraticn_,x0,mu=0.01)\n",
    "\n",
    "print (\"Nb it√©rations\",len(res))\n",
    "print(\"x_calcul√©\",res[-1])\n",
    "print (\"x_exact\",x_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1√®re am√©lioration: le choix d'un pas \"optimal\" √† chaque √©tape par *backtracking* (ou rebroussement avec crit√®re d'Armijo)\n",
    "\n",
    "Vous devriez avoir constat√© que le choix du pas de descente dans le cas constant est crucial pour garantir la convergence de l'algorithme de descente. Dans cette section on s'int√©resse √† un calcul adaptatif du pas de descente qui permet de mieux garantir la convergence de notre algo. Le d√©savantage est le temps que prend d√©sormais chaque it√©ration pour s'ex√©cuter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on veut imposer la d√©croissance de la suite $f({\\bf x}_k)$ ce qui nous assurera la convergence. Mais pour assurer la convergence vers un minimum, il faut imposer un peu plus que simplement de descendre, c'est ce que traduit le crit√®re d'Armijo.\n",
    "\n",
    "On prend ici deux param√®tres $0<\\alpha<0.5$ et $0<\\beta<1$.\n",
    "\n",
    "On cherche un $\\eta$ qui v√©rifie:\n",
    "$$ f({\\bf x}_k+\\eta {\\bf d}_k) < f({\\bf x}_k) + \\alpha \\, \\eta \\, {\\bf d}_k ^T \\, \\nabla f({\\bf x}_k)\\quad (1).$$\n",
    "\n",
    "Un tel $\\eta$ existe d√®s que ${\\bf d}_k$ est une direction de descente (c'est √† dire d√®s que ${\\bf d}_k ^T \\, \\nabla f({\\bf x}_k) \\, <\\, 0$.\n",
    "\n",
    "* On part de $\\eta = 1$\n",
    "\n",
    "* si la condition $(1)$ ci-dessus est v√©rifi√©e on choisit cette valeur de $\\eta$\n",
    "\n",
    "* sinon on change $\\eta$ en $\\beta \\, \\eta$\n",
    "\n",
    "Et on recommence jusqu'√† ce que la condition $(1)$ soit v√©rifi√©e.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. √âcrire une fonction `backtracking` qui permet de calculer le pas par *backtracking* avec crit√®re d'Armijo √† une it√©ration donn√©e. Pour rappel le *backtracking* a deux hyper-param√®tres $\\alpha$ et $\\beta$ que vous mettrez par d√©faut √† $0.4$ et $0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction g√©n√®re la taille du pas optimal v√©rifiant le crit√®re d'Amijo\n",
    "\n",
    "def backtrack(x0, f , dir_x, alpha = 0.4, beta = 0.8):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    x0: point actuel\n",
    "    f: fonction √† minimiser\n",
    "    dir_x: direction dans laquelle on souhaite aller\n",
    "    \n",
    "    Output\n",
    "    valeur du pas optimal\n",
    "    \"\"\"\n",
    "    \n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©thode est donc tr√®s analogue √† la pr√©c√©dente\n",
    "\n",
    "* prendre un point de d√©part au hasard ${\\bf x_0}$.\n",
    "\n",
    "Quand on est au point ${\\bf x_k}$\n",
    "\n",
    "* Calculer le gradient de $f$ en ce point $\\nabla f({\\bf x_k})$\n",
    "\n",
    "* Choisir une direction de descente ${\\bf d}_k = - \\nabla f({\\bf x}_k)$\n",
    "\n",
    "* Trouver $\\eta_k$ par \"backtracking\" dans la direction ${\\bf d}_k$\n",
    "\n",
    "* Avancer dans cette direction et avec ce pas : ${\\bf x}_{k+1} = {\\bf x}_k + \\eta \\, {\\bf d}_k$\n",
    "\n",
    "et on recommence ces √©tapes jusqu'√† ce qu'on arrive √† un point fixe c.a.d. que \n",
    "$|| {\\bf x}_{k+1} - {\\bf x}_k|| < \\varepsilon$ avec $\\varepsilon$ une toute petite valeur.\n",
    "\n",
    "#### 2. Impl√©menter cette m√©thode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_grad_opti (f,x0,eps=1E-6):\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (quadratic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (Rosenbrok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Comparer la descente par backtracking et la descente √† pas constant (√† vous de r√©fl√©chir √† ce que vous voulez comparer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## IV- Changement de direction de descente\n",
    "\n",
    "Maintenant nous ne nous d√©placerons plus n√©cessairement dans la direction de $-\\nabla f({\\bf x_k})$ mais dans une autre direction $d_k$ qui v√©rifiera bien √©videmment $\\langle \\nabla f({\\bf x_k}) , d_k \\rangle < 0$ afin que ce soit bien une direction de descente (et non de mont√©e !!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Choisir une direction de descente selon la plus forte pente en norme $l_1$\n",
    "\n",
    "Nous allons ici choisir la descente de plus forte pente dans le cas de la norme $\\ell_1$ : la direction de descente $d_k$ suit le vecteur de la base canonique de plus grande d√©riv√©e partielle en valeur absolue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\bf d}_k = -\\langle \\nabla f({\\bf x}_k),e_i \\rangle \\, e_i$$\n",
    "o√π $i$ est le plus petit indice tel que:\n",
    "$ \\left| \\dfrac{\\partial f}{\\partial x_i}({\\bf x}_k) \\right| = \\|\\nabla f({\\bf x}_k)\\|_{\\infty}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. √âcrire une fonction `dsgd` qui calcule cette direction de descente de plus forte pente dans le cas de la norme $\\ell_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsgd(f, x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    x: point actuel\n",
    "    f: fonction √† minimiser\n",
    "    \n",
    "    Output\n",
    "    vecteur de direction de descente de gradient maximal en norme l1\n",
    "    \"\"\"\n",
    "    \n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Ecrire la descente √† pas optimal avec cette direction de descente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf x}_{k+1} = {\\bf x}_k + \\eta_k {\\bf d}_k$, o√π $\\mu_k$ est calcul√© de mani√®re optimale par l'algorithme de rebroussement avec crit√®re d'Amijo d√©fini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_l1_opti (f, x0, eps=1E-6):\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (quadratic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (Rosenbrok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Comparer la descente $\\ell_1$ et la descente du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. le gradient conjugu√© (Fletcher-Reeves)\n",
    "\n",
    "Dans la m√©thode du gradient conjugu√©, on modifie la direction de descente en ajoutant √† l'oppos√© du gradient un terme d√©pendant des directions de descente pr√©c√©dentes. Ce choix de descente est fait pour rendre deux directions de descentes orthogonales pour le produit scalaire qui vient de la Hessienne.\n",
    "\n",
    "Ce calcul (qui est direct quand la fonctionnelle est quadratique) peut devenir compliqu√© quand la Hessienne n'est pas directement accessible.\n",
    "\n",
    "Une des m√©thodes les plus populaires pour une fonctionnelle quelconque est celle propos√©e par Fletcher-Reeves. Nous vous invitons √† faire un peu de bibliographie pour trouver comment la direction est choisie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Ecrire la descente √† pas optimal avec cette direction de descente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_FR_opti (f, x0, eps=1E-6):\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (quadratic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (Rosenbrok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. le gradient conjugu√© (Polack-Ribi√®re)\n",
    "\n",
    "Une m√©thode alternative est celle propos√©e par Polack-Ribi√®re. √Ä noter qu'elle est (th√©oriquement) √©quivalente √† la m√©thode de Fletcher-Reeves lorsque la fonctionnelle √† minimiser est quadratique, mais sensiblement plus efficace dans le cas g√©n√©ral (non quadratique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Ecrire la descente √† pas optimal avec cette direction de descente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_PR_opti (f, x0, eps=1E-6):\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (quadratic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (Rosenbrok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. visualiser une comparaison des diff√©rents algorithmes de descente et leur sensibilit√© par rapport au conditionnement\n",
    "\n",
    "Nous vous proposons de visualiser le comportement de ces diff√©rents algorithmes de descente.\n",
    "\n",
    "Tout d'abord en dimension 2, vous pouvez tracer la succession des points $x_k$ pour les algorithmes impl√©ment√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2\n",
    "A,b = create_system(dim,cond=3.)\n",
    "x_exact=np.zeros(dim)+1.\n",
    "b=A@x_exact\n",
    "\n",
    "x0=np.zeros(dim)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "Liste_method=(desc_grad_const,desc_grad_opti,desc_l1_opti,desc_FR_opti,desc_PR_opti)\n",
    "\n",
    "for method in Liste_method:\n",
    "\n",
    "    if method == desc_grad_const:\n",
    "        res = method(quadraticn_,x0,eta=0.9/cond)\n",
    "    else:\n",
    "        res = method(quadraticn_,x0)\n",
    "\n",
    "    plt.plot(res[:,0], res[:,1],label=method)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "nb_pts, x_mi, x_ma, y_mi, y_ma = 200, -0.1, 1.1, -0.1, 1.3\n",
    "\n",
    "x, y = np.linspace(x_mi, x_ma, nb_pts), np.linspace(y_mi, y_ma, nb_pts)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z=np.zeros((nb_pts,nb_pts))\n",
    "\n",
    "for i in range (nb_pts):\n",
    "    for j in range (nb_pts):\n",
    "        Z[i,j]=quadraticn_(np.array([X[i,j],Y[i,j]]))\n",
    "        \n",
    "        \n",
    "plt.contour(X, Y, Z,25) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dimension n plus grande (par exemple n=10), vous pourrez tracer le nombre d'it√©rations n√©cessaires pour converger en fonction du conditionnement (5,10,50,100,500,1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi regarder pour une certaine valeur de conditionnement, comment varie la distance √† la solution exacte en fonction de l'it√©ration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. comportement si la fonction √† optimiser n'est pas convexe sur $\\mathbb{R}^2$ et n'admet pas un minimum global\n",
    "\n",
    "Nous vous proposons de regarder le comportement de ces diff√©rents algorithmes de descente dans le cas de quelques fonctions introduites au d√©but du TP, telles que multitrous2, cubic2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (multitrous2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (cubic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V Acc√©l√©rations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des strat√©gies standards d'acc√©l√©ration de descente de gradients ; on en invente m√™me tous les ans. Il n'y a pas n√©cessairement de relation d'ordre entre celles-ci, certaines sont plus adapt√©es que d'autres √† des probl√®mes sp√©cifiques et inversement. On vous propose d'impl√©menter 3 m√©thodes \"simples\" : *momentum*, *Nesterov* et *Adagrad* (*Adaptive gradient*). Contrairement aux deux premi√®res pour lesquelles le pas reste fixe en fonction des variables √† optimiser, *Adagrad* adapte le pas d'une variable √† l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Impl√©menter la *Momentum Optimisation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Impl√©menter la *Nesterov Optimisation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Impl√©menter la *Adagrad Optimisation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Comparer l'acc√©l√©ration choisie avec les m√©thodes pr√©c√©dentes pr√©c√©dentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI - \"Les\" m√©thodes de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. la m√©thode de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les m√©thodes de descente pr√©c√©dentes, on cherchait √† chaque √©tape √† minimiser le D√©veloppement Limit√© d'ordre 1 de J au voisinage de ${\\bf x_k}$: $h \\mapsto f({\\bf x_k}) + \\nabla f({\\bf x_k}) ^T h$.\n",
    "\n",
    "Dans la m√©thode de Newton, on cherche √† minimiser le D√©veloppement Limit√© d'ordre 2 de J au voisinage de ${\\bf x}_k$ : $h \\mapsto f({\\bf x}_k) + \\nabla f({\\bf x}_k) ^T h + \\dfrac{1}{2}h^T H_f({\\bf x}_k) h$ o√π $H_f({\\bf x}_k)$ est la Hessienne de $f$ au point ${\\bf x}_k$.\n",
    "\n",
    "A quelle direction de descente, cette minimisation nous am√®ne-t-elle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©ponse:\n",
    "\n",
    "\n",
    "On passe de la direction ${\\bf d}_k$ √† la direction ${\\bf d}_{k+1}$ par la relation de r√©currence:\n",
    "$${\\bf d}_{k+1}=-H_f({\\bf x}_k)^{-1} \\, \\nabla f({\\bf x}_k) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Impl√©menter la m√©thode de Newton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En apparence, impl√©menter la m√©thode de Newton (qui converge tr√®s rapidement) ne semble pas diff√©rent que les m√©thodes de descente vues pr√©c√©demment\n",
    "\n",
    "En r√©alit√© cette m√©thode n'est pas facile √† impl√©menter et surtout co√ªteuse car il faut calculer √† chaque it√©ration la Hessienne de $f$ au point ${\\bf x}_k)$ et ensuite l'inverser.\n",
    "\n",
    "Vous pouvez essayer de l'impl√©menter dans le cas particulier de la fonction quadraticn_ car alors la matrice Hessienne est constante et ne d√©pend donc pas du point ${\\bf x}_k)$, ce qui vous permet de calculer une fois pour toute son inverse (par le module linalg de numpy par exemple).\n",
    "\n",
    "Par contre impl√©menter cette m√©thode dans le cas d'une fonction dont on ne conna√Æt pas explicitement la Hessienne, et surtout l'inverse de cette derni√®re est √† d√©conseiller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. la m√©thode de quasi-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour √©viter les inconv√©nients de la m√©thode de Newton, en particulier lorsque la Hessienne n'est plus constante, une id√©e est d'introduire une suite de matrices $H_k$ qui sont une approximation de $\\bf {l'inverse}$ de la Hessienne au point ${\\bf x}_k$.\n",
    "\n",
    "Cette m√©thode qui s'appelle m√©thode BFGS (des initiales des auteurs qui l'ont introduite), a l'avantage (√©norme) de ne pas n√©cessiter le calcul de la Hessienne ou de son inverse. on retrouve donc en terme d'impl√©mentation la simplicit√© des m√©thodes de descente, tout en ayant les propri√©t√©s de convergence de la m√©thode de Newton.\n",
    "\n",
    "L'algorithme le d√©crivant est le suivant (tr√®s analogue aux algorithmes de descente):\n",
    "\n",
    "* prendre un point de d√©part quelconque ${\\bf x}_0$\n",
    "* prendre une matrice $H_0$ quelconque, par exemple $H_0= I_n$ \n",
    "\n",
    "Quand on est au point ${\\bf x}_k$\n",
    "\n",
    "* Choisir une direction de descente ${\\bf d}_k = -H_k \\, \\nabla f({\\bf x}_k)$\n",
    "\n",
    "* Trouver $\\eta_k$ par \"backtracking\" dans la direction ${\\bf d}_k$\n",
    "\n",
    "* Avancer dans cette direction et avec ce pas : ${\\bf x}_{k+1} = {\\bf x}_k + \\eta_k \\, {\\bf d}_k$\n",
    "\n",
    "* Calculer $H_{k+1}$ en fonction de $H_{k+1}$, $f({\\bf x}_k)$, $f({\\bf x}_{k+1})$, $\\nabla f({\\bf x}_k)$ et $\\nabla f({\\bf x}_{k+1})$.\n",
    "\n",
    "et on recommence ces √©tapes jusqu'√† ce qu'on arrive √† un point fixe c.a.d. que \n",
    "$|| {\\bf x}_{k+1} - {\\bf x}_k|| < \\varepsilon$ avec $\\varepsilon$ une toute petite valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formule qui donne la suite des matrices $H_{k}$ semble √† premi√®re vue horrible. Mais elle se programme en Python en 1 ligne en ne faisant que des multiplications de matrices et de vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Impl√©menter la m√©thode de Quasi-Newton BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_BFGS_opti (f, x0, eps=1E-6):\n",
    "    # üë∑ √Ä VOUS DE JOUER üë∑\n",
    "    # return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tester cette descente sur un cas simple (prendre par exemple en dimension n=10, la fonction quadraticn_ dans un cas o√π vous connaissez la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (quadratic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (Rosenbrok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑ (cubic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Comparer la m√©thode de quasi-Newton aux descentes pr√©c√©dentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üë∑ √Ä VOUS DE JOUER üë∑"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
